{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b4ac32",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m \u001b[38;5;66;03m# for making figures\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/__init__.py:53\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules.get(\u001b[33m\"\u001b[39m\u001b[33mtorch._meta_registrations\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     49\u001b[39m     _functionalize_sync \u001b[38;5;28;01mas\u001b[39;00m _sync,\n\u001b[32m     50\u001b[39m     _import_dotted_name,\n\u001b[32m     51\u001b[39m     classproperty,\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     54\u001b[39m     get_file_path,\n\u001b[32m     55\u001b[39m     prepare_multiprocessing_environment,\n\u001b[32m     56\u001b[39m     USE_GLOBAL_DEPS,\n\u001b[32m     57\u001b[39m     USE_RTLD_GLOBAL_WITH_LIBTORCH,\n\u001b[32m     58\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _running_with_deploy():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/_utils_internal.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_strobelight\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_time_profiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrobelightCompileTimeProfiler\n\u001b[32m     14\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m _P = ParamSpec(\u001b[33m\"\u001b[39m\u001b[33m_P\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:990\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1086\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1185\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fde312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4aff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:3]:\n",
    "  \n",
    "  print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad66beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2)) # THIS IS THE EMBEDDING MATRIX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[5] # the embedding for character 'e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C # one-hot encoding for 'e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above cells are same because we are using a one-hot encoding of the character 'e' and multiplying it with the embedding matrix C and rest are zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[[5, 6, 7]].shape # embeddings for 'e', 'f', 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[torch.tensor([5, 6, 7,7,7])] # embeddings for 'e', 'f', 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X].shape ,C[X][1] # embeddings for all characters in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor\tShape\tMeaning\n",
    "# C\t(vocab_size, embedding_dim)\tEmbedding matrix\n",
    "# X\t(batch_size, sequence_len)\tBatch of token indices\n",
    "# C[X]\t(batch_size, sequence_len, embedding_dim)\tEmbeddings for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[13][2]  , C[X][13][2] , C[1]   # embedding for the 2nd character in the 13th example the value is 1 because the character is 'a' and the embedding for 'a' is the first row in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cf10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # embeddings for all characters in X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((2*3, 100)) # weights for the first layer 2 is embedding dim and we have 3 of them \n",
    "b1 = torch.randn(100) # bias for the first layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d101a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([emb[:,0,: ] , emb[:,1,: ], emb[:,2,: ]] ,dim=1).shape # concatenating the embeddings for the first, second and third character in each example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361fdec",
   "metadata": {},
   "source": [
    "### Example: Using `torch.unbind`\n",
    "\n",
    "The function `torch.unbind(tensor, dim)` removes a given dimension and returns a tuple of slices along that dimension.\n",
    "\n",
    "For example, given the tensor `emb` of shape `(16, 3, 2)`:\n",
    "\n",
    "```python\n",
    "emb.shape\n",
    "# Output: torch.Size([16, 3, 2])\n",
    "```\n",
    "\n",
    "Unbinding along `dim=1`:\n",
    "\n",
    "```python\n",
    "emb0, emb1, emb2 = torch.unbind(emb, dim=1)\n",
    "print(emb0.shape, emb1.shape, emb2.shape)\n",
    "# Output: torch.Size([16, 2]) torch.Size([16, 2]) torch.Size([16, 2])\n",
    "```\n",
    "\n",
    "This splits `emb` into three tensors, each corresponding to one position in the context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be80958",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unbind(emb,dim=1)# unbinding the embeddings along the second dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25daa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(torch.unbind(emb,dim=1), dim=1).shape # concatenating the embeddings for the first, second and third character in each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02afb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(3, 6) , a.view (2,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf01a50",
   "metadata": {},
   "source": [
    "emb.view(-1,6)@ W1 shape is 16 x 100\n",
    " b1s shape is               1  x 100 it will be broadcasated and added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = emb.view(-1,6)@ W1 + b1 # matrix multiplication with the weights and adding the bias\n",
    "h.shape # shape of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e016b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(h) # activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27)) # weights for the second layer\n",
    "b2 = torch.randn(27) # bias for the second layer\n",
    "logits = h @ W2 + b2 # output layer\n",
    "logits.shape # shape of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp() # converting logits to counts\n",
    "prob = counts / counts.sum(1, keepdim=True) # converting counts to probabilities\n",
    "prob.shape , prob[0].sum() # shape of the probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08439e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dafd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[torch.arange(len(Y)), Y] # getting the probabilities for the correct characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78262861",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -(torch.log(prob[torch.arange(len(Y)), Y]).mean()) # calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e732ae",
   "metadata": {},
   "source": [
    "More respectable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d56816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape # shapes of the input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8017e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g  = torch.Generator().manual_seed(2147483647) # random number generator for reproducibility\n",
    "C = torch.randn((27, 2), generator=g) # re-initializing the embedding matrix with a fixed seed\n",
    "W1 = torch.randn((2*3, 100), generator=g) # re-initializing the weights for the first layer\n",
    "b1 = torch.randn(100, generator=g) # re-initializing the bias for the first layer\n",
    "W2 = torch.randn((100, 27), generator=g) # re-initializing the weights for the second layer\n",
    "b2 = torch.randn(27, generator=g) # re-initializing the bias for the second layer\n",
    "parameters = [C, W1, b1, W2, b2] # list of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # embeddings for all characters in X\n",
    "h = emb.view(-1, 6) @ W1 + b1 # matrix multiplication with the weights and adding the bias\n",
    "h = torch.tanh(h) # activation function\n",
    "logits = h @ W2 + b2 # output layer\n",
    "counts = logits.exp() # converting logits to counts\n",
    "prob = counts / counts.sum(1, keepdim=True) # converting counts to probabilities\n",
    "loss = -(torch.log(prob[torch.arange(len(Y)), Y]).mean()) # calculating the loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc85217",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(logits, Y) # calculating the loss using cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6702f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "  \n",
    "#   print(w)\"\"\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g  = torch.Generator().manual_seed(2147483647) # random number generator for reproducibility\n",
    "C = torch.randn((27, 2), generator=g) # re-initializing the embedding matrix with a fixed seed\n",
    "W1 = torch.randn((2*3, 100), generator=g) # re-initializing the weights for the first layer\n",
    "b1 = torch.randn(100, generator=g) # re-initializing the bias for the first layer\n",
    "W2 = torch.randn((100, 27), generator=g) # re-initializing the weights for the second layer\n",
    "b2 = torch.randn(27, generator=g) # re-initializing the bias for the second layer\n",
    "parameters = [C, W1, b1, W2, b2] # list of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea703a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True # setting requires_grad to True for all parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b03450",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1): # training loop\n",
    "    emb = C[X] # embeddings for all characters in X\n",
    "    h = emb.view(-1, 6) @ W1 + b1 # matrix multiplication with the weights and adding the bias\n",
    "    h = torch.tanh(h) # activation function\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Y) # calculating the loss\n",
    "    print(loss.item()) # printing the loss\n",
    "    for p in parameters:\n",
    "        p.grad = None # zeroing the gradients\n",
    "    \n",
    "    loss.backward() # backpropagation\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad # updating the parameters with a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0,X.shape[0], (32,)) # generating random indices to sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db563d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "  \n",
    "#   print(w)\"\"\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g  = torch.Generator().manual_seed(2147483647) # random number generator for reproducibility\n",
    "C = torch.randn((27, 2), generator=g) # re-initializing the embedding matrix with a fixed seed\n",
    "W1 = torch.randn((2*3, 100), generator=g) # re-initializing the weights for the first layer\n",
    "b1 = torch.randn(100, generator=g) # re-initializing the bias for the first layer\n",
    "W2 = torch.randn((100, 27), generator=g) # re-initializing the weights for the second layer\n",
    "b2 = torch.randn(27, generator=g) # re-initializing the bias for the second layer\n",
    "parameters = [C, W1, b1, W2, b2] # list of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10809bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True # setting requires_grad to True for all parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # learning rate exponential decay\n",
    "lrs = 10 ** lre # learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "for i in range(100): # training loop\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # generating random indices to sample from the dataset\n",
    "    emb = C[X[ix]] # embeddings for all characters in X\n",
    "    h = emb.view(-1, 6) @ W1 + b1 # matrix multiplication with the weights and adding the bias\n",
    "    h = torch.tanh(h) # activation function\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # calculating the loss\n",
    "    print(loss.item()) # printing the loss\n",
    "    for p in parameters:\n",
    "        p.grad = None # zeroing the gradients\n",
    "    \n",
    "    loss.backward() # backpropagation\n",
    "    lr =lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad # updating the parameters with a learning rate of 0.1\n",
    "    #track \n",
    "    lri.append(lrs[i]) # tracking the learning rate\n",
    "    lossi.append(loss.item()) # tracking the loss\n",
    "plt.plot(lri, lossi) # plotting the learning rate vs loss\n",
    "print(loss.item()) # printing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # embeddings for all characters in X\n",
    "h = emb.view(-1, 6) @ W1 + b1 # matrix multiplication with the weights and adding the bias\n",
    "h = torch.tanh(h) # activation function\n",
    "logits = h @ W2 + b2 # output layer\n",
    "loss = F.cross_entropy(logits, Y) # calculating the loss\n",
    "print(loss.item()) # printing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
