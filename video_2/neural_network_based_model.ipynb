{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a785735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b4c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn',\n",
       " 'abigail',\n",
       " 'emily',\n",
       " 'elizabeth',\n",
       " 'mila',\n",
       " 'ella',\n",
       " 'avery',\n",
       " 'sofia',\n",
       " 'camila',\n",
       " 'aria',\n",
       " 'scarlett',\n",
       " 'victoria',\n",
       " 'madison',\n",
       " 'luna',\n",
       " 'grace',\n",
       " 'chloe',\n",
       " 'penelope',\n",
       " 'layla',\n",
       " 'riley',\n",
       " 'zoey',\n",
       " 'nora',\n",
       " 'lily',\n",
       " 'eleanor',\n",
       " 'hannah',\n",
       " 'lillian',\n",
       " 'addison',\n",
       " 'aubrey',\n",
       " 'ellie',\n",
       " 'stella',\n",
       " 'natalie',\n",
       " 'zoe',\n",
       " 'leah',\n",
       " 'hazel',\n",
       " 'violet',\n",
       " 'aurora',\n",
       " 'savannah',\n",
       " 'audrey',\n",
       " 'brooklyn',\n",
       " 'bella',\n",
       " 'claire',\n",
       " 'skylar',\n",
       " 'lucy',\n",
       " 'paisley',\n",
       " 'everly',\n",
       " 'anna',\n",
       " 'caroline',\n",
       " 'nova',\n",
       " 'genesis',\n",
       " 'emilia',\n",
       " 'kennedy',\n",
       " 'samantha',\n",
       " 'maya',\n",
       " 'willow',\n",
       " 'kinsley',\n",
       " 'naomi',\n",
       " 'aaliyah',\n",
       " 'elena',\n",
       " 'sarah',\n",
       " 'ariana',\n",
       " 'allison',\n",
       " 'gabriella',\n",
       " 'alice',\n",
       " 'madelyn',\n",
       " 'cora',\n",
       " 'ruby',\n",
       " 'eva',\n",
       " 'serenity',\n",
       " 'autumn',\n",
       " 'adeline',\n",
       " 'hailey',\n",
       " 'gianna',\n",
       " 'valentina',\n",
       " 'isla',\n",
       " 'eliana',\n",
       " 'quinn',\n",
       " 'nevaeh',\n",
       " 'ivy',\n",
       " 'sadie',\n",
       " 'piper',\n",
       " 'lydia',\n",
       " 'alexa',\n",
       " 'josephine',\n",
       " 'emery',\n",
       " 'julia',\n",
       " 'delilah',\n",
       " 'arianna',\n",
       " 'vivian',\n",
       " 'kaylee',\n",
       " 'sophie',\n",
       " 'brielle',\n",
       " 'madeline',\n",
       " 'peyton',\n",
       " 'rylee',\n",
       " 'clara',\n",
       " 'hadley',\n",
       " 'melanie',\n",
       " 'mackenzie',\n",
       " 'reagan',\n",
       " 'adalynn',\n",
       " 'liliana',\n",
       " 'aubree',\n",
       " 'jade',\n",
       " 'katherine',\n",
       " 'isabelle',\n",
       " 'natalia',\n",
       " 'raelynn',\n",
       " 'maria',\n",
       " 'athena',\n",
       " 'ximena',\n",
       " 'arya',\n",
       " 'leilani',\n",
       " 'taylor',\n",
       " 'faith',\n",
       " 'rose',\n",
       " 'kylie',\n",
       " 'alexandra',\n",
       " 'mary',\n",
       " 'margaret',\n",
       " 'lyla',\n",
       " 'ashley',\n",
       " 'amaya',\n",
       " 'eliza',\n",
       " 'brianna',\n",
       " 'bailey',\n",
       " 'andrea',\n",
       " 'khloe',\n",
       " 'jasmine',\n",
       " 'melody',\n",
       " 'iris',\n",
       " 'isabel',\n",
       " 'norah',\n",
       " 'annabelle',\n",
       " 'valeria',\n",
       " 'emerson',\n",
       " 'adalyn',\n",
       " 'ryleigh',\n",
       " 'eden',\n",
       " 'emersyn',\n",
       " 'anastasia',\n",
       " 'kayla',\n",
       " 'alyssa',\n",
       " 'juliana',\n",
       " 'charlie',\n",
       " 'esther',\n",
       " 'ariel',\n",
       " 'cecilia',\n",
       " 'valerie',\n",
       " 'alina',\n",
       " 'molly',\n",
       " 'reese',\n",
       " 'aliyah',\n",
       " 'lilly',\n",
       " 'parker',\n",
       " 'finley',\n",
       " 'morgan',\n",
       " 'sydney',\n",
       " 'jordyn',\n",
       " 'eloise',\n",
       " 'trinity',\n",
       " 'daisy',\n",
       " 'kimberly',\n",
       " 'lauren',\n",
       " 'genevieve',\n",
       " 'sara',\n",
       " 'arabella',\n",
       " 'harmony',\n",
       " 'elise',\n",
       " 'remi',\n",
       " 'teagan',\n",
       " 'alexis',\n",
       " 'london',\n",
       " 'sloane',\n",
       " 'laila',\n",
       " 'lucia',\n",
       " 'diana',\n",
       " 'juliette',\n",
       " 'sienna',\n",
       " 'elliana',\n",
       " 'londyn',\n",
       " 'ayla',\n",
       " 'callie',\n",
       " 'gracie',\n",
       " 'josie',\n",
       " 'amara',\n",
       " 'jocelyn',\n",
       " 'daniela',\n",
       " 'everleigh',\n",
       " 'mya',\n",
       " 'rachel',\n",
       " 'summer',\n",
       " 'alana',\n",
       " 'brooke',\n",
       " 'alaina',\n",
       " 'mckenzie',\n",
       " 'catherine',\n",
       " 'amy',\n",
       " 'presley',\n",
       " 'journee',\n",
       " 'rosalie',\n",
       " 'ember',\n",
       " 'brynlee',\n",
       " 'rowan',\n",
       " 'joanna',\n",
       " 'paige',\n",
       " 'rebecca',\n",
       " 'ana',\n",
       " 'sawyer',\n",
       " 'mariah',\n",
       " 'nicole',\n",
       " 'brooklynn',\n",
       " 'payton',\n",
       " 'marley',\n",
       " 'fiona',\n",
       " 'georgia',\n",
       " 'lila',\n",
       " 'harley',\n",
       " 'adelyn',\n",
       " 'alivia',\n",
       " 'noelle',\n",
       " 'gemma',\n",
       " 'vanessa',\n",
       " 'journey',\n",
       " 'makayla',\n",
       " 'angelina',\n",
       " 'adaline',\n",
       " 'catalina',\n",
       " 'alayna',\n",
       " 'julianna',\n",
       " 'leila',\n",
       " 'lola',\n",
       " 'adriana',\n",
       " 'june',\n",
       " 'juliet',\n",
       " 'jayla',\n",
       " 'river',\n",
       " 'tessa',\n",
       " 'lia',\n",
       " 'dakota',\n",
       " 'delaney',\n",
       " 'selena',\n",
       " 'blakely',\n",
       " 'ada',\n",
       " 'camille',\n",
       " 'zara',\n",
       " 'malia',\n",
       " 'hope',\n",
       " 'samara',\n",
       " 'vera',\n",
       " 'mckenna',\n",
       " 'briella',\n",
       " 'izabella',\n",
       " 'hayden',\n",
       " 'raegan',\n",
       " 'michelle',\n",
       " 'angela',\n",
       " 'ruth',\n",
       " 'freya',\n",
       " 'kamila',\n",
       " 'vivienne',\n",
       " 'aspen',\n",
       " 'olive',\n",
       " 'kendall',\n",
       " 'elaina',\n",
       " 'thea',\n",
       " 'kali',\n",
       " 'destiny',\n",
       " 'amiyah',\n",
       " 'evangeline',\n",
       " 'cali',\n",
       " 'blake',\n",
       " 'elsie',\n",
       " 'juniper',\n",
       " 'alexandria',\n",
       " 'myla',\n",
       " 'ariella',\n",
       " 'kate',\n",
       " 'mariana',\n",
       " 'lilah',\n",
       " 'charlee',\n",
       " 'daleyza',\n",
       " 'nyla',\n",
       " 'jane',\n",
       " 'maggie',\n",
       " 'zuri',\n",
       " 'aniyah',\n",
       " 'lucille',\n",
       " 'leia',\n",
       " 'melissa',\n",
       " 'adelaide',\n",
       " 'amina',\n",
       " 'giselle',\n",
       " 'lena',\n",
       " 'camilla',\n",
       " 'miriam',\n",
       " 'millie',\n",
       " 'brynn',\n",
       " 'gabrielle',\n",
       " 'sage',\n",
       " 'annie',\n",
       " 'logan',\n",
       " 'lilliana',\n",
       " 'haven',\n",
       " 'jessica',\n",
       " 'kaia',\n",
       " 'magnolia',\n",
       " 'amira',\n",
       " 'adelynn',\n",
       " 'makenzie',\n",
       " 'stephanie',\n",
       " 'nina',\n",
       " 'phoebe',\n",
       " 'arielle',\n",
       " 'evie',\n",
       " 'lyric',\n",
       " 'alessandra',\n",
       " 'gabriela',\n",
       " 'paislee',\n",
       " 'raelyn',\n",
       " 'madilyn',\n",
       " 'paris',\n",
       " 'makenna',\n",
       " 'kinley',\n",
       " 'gracelyn',\n",
       " 'talia',\n",
       " 'maeve',\n",
       " 'rylie',\n",
       " 'kiara',\n",
       " 'evelynn',\n",
       " 'brinley',\n",
       " 'jacqueline',\n",
       " 'laura',\n",
       " 'gracelynn',\n",
       " 'lexi',\n",
       " 'ariah',\n",
       " 'fatima',\n",
       " 'jennifer',\n",
       " 'kehlani',\n",
       " 'alani',\n",
       " 'ariyah',\n",
       " 'luciana',\n",
       " 'allie',\n",
       " 'heidi',\n",
       " 'maci',\n",
       " 'phoenix',\n",
       " 'felicity',\n",
       " 'joy',\n",
       " 'kenzie',\n",
       " 'veronica',\n",
       " 'margot',\n",
       " 'addilyn',\n",
       " 'lana',\n",
       " 'cassidy',\n",
       " 'remington',\n",
       " 'saylor',\n",
       " 'ryan',\n",
       " 'keira',\n",
       " 'harlow',\n",
       " 'miranda',\n",
       " 'angel',\n",
       " 'amanda',\n",
       " 'daniella',\n",
       " 'royalty',\n",
       " 'gwendolyn',\n",
       " 'ophelia',\n",
       " 'heaven',\n",
       " 'jordan',\n",
       " 'madeleine',\n",
       " 'esmeralda',\n",
       " 'kira',\n",
       " 'miracle',\n",
       " 'elle',\n",
       " 'amari',\n",
       " 'danielle',\n",
       " 'daphne',\n",
       " 'willa',\n",
       " 'haley',\n",
       " 'gia',\n",
       " 'kaitlyn',\n",
       " 'oakley',\n",
       " 'kailani',\n",
       " 'winter',\n",
       " 'alicia',\n",
       " 'serena',\n",
       " 'nadia',\n",
       " 'aviana',\n",
       " 'demi',\n",
       " 'jada',\n",
       " 'braelynn',\n",
       " 'dylan',\n",
       " 'ainsley',\n",
       " 'alison',\n",
       " 'camryn',\n",
       " 'avianna',\n",
       " 'bianca',\n",
       " 'skyler',\n",
       " 'scarlet',\n",
       " 'maddison',\n",
       " 'nylah',\n",
       " 'sarai',\n",
       " 'regina',\n",
       " 'dahlia',\n",
       " 'nayeli',\n",
       " 'raven',\n",
       " 'helen',\n",
       " 'adrianna',\n",
       " 'averie',\n",
       " 'skye',\n",
       " 'kelsey',\n",
       " 'tatum',\n",
       " 'kensley',\n",
       " 'maliyah',\n",
       " 'erin',\n",
       " 'viviana',\n",
       " 'jenna',\n",
       " 'anaya',\n",
       " 'carolina',\n",
       " 'shelby',\n",
       " 'sabrina',\n",
       " 'mikayla',\n",
       " 'annalise',\n",
       " 'octavia',\n",
       " 'lennon',\n",
       " 'blair',\n",
       " 'carmen',\n",
       " 'yaretzi',\n",
       " 'kennedi',\n",
       " 'mabel',\n",
       " 'zariah',\n",
       " 'kyla',\n",
       " 'christina',\n",
       " 'selah',\n",
       " 'celeste',\n",
       " 'eve',\n",
       " 'mckinley',\n",
       " 'milani',\n",
       " 'frances',\n",
       " 'jimena',\n",
       " 'kylee',\n",
       " 'leighton',\n",
       " 'katie',\n",
       " 'aitana',\n",
       " 'kayleigh',\n",
       " 'sierra',\n",
       " 'kathryn',\n",
       " 'rosemary',\n",
       " 'jolene',\n",
       " 'alondra',\n",
       " 'elisa',\n",
       " 'helena',\n",
       " 'charleigh',\n",
       " 'hallie',\n",
       " 'lainey',\n",
       " 'avah',\n",
       " 'jazlyn',\n",
       " 'kamryn',\n",
       " 'mira',\n",
       " 'cheyenne',\n",
       " 'francesca',\n",
       " 'antonella',\n",
       " 'wren',\n",
       " 'chelsea',\n",
       " 'amber',\n",
       " 'emory',\n",
       " 'lorelei',\n",
       " 'nia',\n",
       " 'abby',\n",
       " 'april',\n",
       " 'emelia',\n",
       " 'carter',\n",
       " 'aylin',\n",
       " 'cataleya',\n",
       " 'bethany',\n",
       " 'marlee',\n",
       " 'carly',\n",
       " 'kaylani',\n",
       " 'emely',\n",
       " 'liana',\n",
       " 'madelynn',\n",
       " 'cadence',\n",
       " 'matilda',\n",
       " 'sylvia',\n",
       " 'myra',\n",
       " 'fernanda',\n",
       " 'oaklyn',\n",
       " 'elianna',\n",
       " 'hattie',\n",
       " 'dayana',\n",
       " 'kendra',\n",
       " 'maisie',\n",
       " 'malaysia',\n",
       " 'kara',\n",
       " 'katelyn',\n",
       " 'maia',\n",
       " 'celine',\n",
       " 'cameron',\n",
       " 'renata',\n",
       " 'jayleen',\n",
       " 'charli',\n",
       " 'emmalyn',\n",
       " 'holly',\n",
       " 'azalea',\n",
       " 'leona',\n",
       " 'alejandra',\n",
       " 'bristol',\n",
       " 'collins',\n",
       " 'imani',\n",
       " 'meadow',\n",
       " 'alexia',\n",
       " 'edith',\n",
       " 'kaydence',\n",
       " 'leslie',\n",
       " 'lilith',\n",
       " 'kora',\n",
       " 'aisha',\n",
       " 'meredith',\n",
       " 'danna',\n",
       " 'wynter',\n",
       " 'emberly',\n",
       " 'julieta',\n",
       " 'michaela',\n",
       " 'alayah',\n",
       " 'jemma',\n",
       " 'reign',\n",
       " 'colette',\n",
       " 'kaliyah',\n",
       " 'elliott',\n",
       " 'johanna',\n",
       " 'remy',\n",
       " 'sutton',\n",
       " 'emmy',\n",
       " 'virginia',\n",
       " 'briana',\n",
       " 'oaklynn',\n",
       " 'adelina',\n",
       " 'everlee',\n",
       " 'megan',\n",
       " 'angelica',\n",
       " 'justice',\n",
       " 'mariam',\n",
       " 'khaleesi',\n",
       " 'macie',\n",
       " 'karsyn',\n",
       " 'alanna',\n",
       " 'aleah',\n",
       " 'mae',\n",
       " 'mallory',\n",
       " 'esme',\n",
       " 'skyla',\n",
       " 'madilynn',\n",
       " 'charley',\n",
       " 'allyson',\n",
       " 'hanna',\n",
       " 'shiloh',\n",
       " 'henley',\n",
       " 'macy',\n",
       " 'maryam',\n",
       " 'ivanna',\n",
       " 'ashlynn',\n",
       " 'lorelai',\n",
       " 'amora',\n",
       " 'ashlyn',\n",
       " 'sasha',\n",
       " 'baylee',\n",
       " 'beatrice',\n",
       " 'itzel',\n",
       " 'priscilla',\n",
       " 'marie',\n",
       " 'jayda',\n",
       " 'liberty',\n",
       " 'rory',\n",
       " 'alessia',\n",
       " 'alaia',\n",
       " 'janelle',\n",
       " 'kalani',\n",
       " 'gloria',\n",
       " 'sloan',\n",
       " 'dorothy',\n",
       " 'greta',\n",
       " 'julie',\n",
       " 'zahra',\n",
       " 'savanna',\n",
       " 'annabella',\n",
       " 'poppy',\n",
       " 'amalia',\n",
       " 'zaylee',\n",
       " 'cecelia',\n",
       " 'coraline',\n",
       " 'kimber',\n",
       " 'emmie',\n",
       " 'anne',\n",
       " 'karina',\n",
       " 'kassidy',\n",
       " 'kynlee',\n",
       " 'monroe',\n",
       " 'anahi',\n",
       " 'jaliyah',\n",
       " 'jazmin',\n",
       " 'maren',\n",
       " 'monica',\n",
       " 'siena',\n",
       " 'marilyn',\n",
       " 'reyna',\n",
       " 'kyra',\n",
       " 'lilian',\n",
       " 'jamie',\n",
       " 'melany',\n",
       " 'alaya',\n",
       " 'ariya',\n",
       " 'kelly',\n",
       " 'rosie',\n",
       " 'adley',\n",
       " 'dream',\n",
       " 'jaylah',\n",
       " 'laurel',\n",
       " 'jazmine',\n",
       " 'mina',\n",
       " 'karla',\n",
       " 'bailee',\n",
       " 'aubrie',\n",
       " 'katalina',\n",
       " 'melina',\n",
       " 'harlee',\n",
       " 'elliot',\n",
       " 'hayley',\n",
       " 'elaine',\n",
       " 'karen',\n",
       " 'dallas',\n",
       " 'irene',\n",
       " 'lylah',\n",
       " 'ivory',\n",
       " 'chaya',\n",
       " 'rosa',\n",
       " 'aleena',\n",
       " 'braelyn',\n",
       " 'nola',\n",
       " 'alma',\n",
       " 'leyla',\n",
       " 'pearl',\n",
       " 'addyson',\n",
       " 'roselyn',\n",
       " 'lacey',\n",
       " 'lennox',\n",
       " 'reina',\n",
       " 'aurelia',\n",
       " 'noa',\n",
       " 'janiyah',\n",
       " 'jessie',\n",
       " 'madisyn',\n",
       " 'saige',\n",
       " 'alia',\n",
       " 'tiana',\n",
       " 'astrid',\n",
       " 'cassandra',\n",
       " 'kyleigh',\n",
       " 'romina',\n",
       " 'stevie',\n",
       " 'haylee',\n",
       " 'zelda',\n",
       " 'lillie',\n",
       " 'aileen',\n",
       " 'brylee',\n",
       " 'eileen',\n",
       " 'yara',\n",
       " 'ensley',\n",
       " 'lauryn',\n",
       " 'giuliana',\n",
       " 'livia',\n",
       " 'anya',\n",
       " 'mikaela',\n",
       " 'palmer',\n",
       " 'lyra',\n",
       " 'mara',\n",
       " 'marina',\n",
       " 'kailey',\n",
       " 'liv',\n",
       " 'clementine',\n",
       " 'kenna',\n",
       " 'briar',\n",
       " 'emerie',\n",
       " 'galilea',\n",
       " 'tiffany',\n",
       " 'bonnie',\n",
       " 'elyse',\n",
       " 'cynthia',\n",
       " 'frida',\n",
       " 'kinslee',\n",
       " 'tatiana',\n",
       " 'joelle',\n",
       " 'armani',\n",
       " 'jolie',\n",
       " 'nalani',\n",
       " 'rayna',\n",
       " 'yareli',\n",
       " 'meghan',\n",
       " 'rebekah',\n",
       " 'addilynn',\n",
       " 'faye',\n",
       " 'zariyah',\n",
       " 'lea',\n",
       " 'aliza',\n",
       " 'julissa',\n",
       " 'lilyana',\n",
       " 'anika',\n",
       " 'kairi',\n",
       " 'aniya',\n",
       " 'noemi',\n",
       " 'angie',\n",
       " 'crystal',\n",
       " 'bridget',\n",
       " 'ari',\n",
       " 'davina',\n",
       " 'amelie',\n",
       " 'amirah',\n",
       " 'annika',\n",
       " 'elora',\n",
       " 'xiomara',\n",
       " 'linda',\n",
       " 'hana',\n",
       " 'laney',\n",
       " 'mercy',\n",
       " 'hadassah',\n",
       " 'madalyn',\n",
       " 'louisa',\n",
       " 'simone',\n",
       " 'kori',\n",
       " 'jillian',\n",
       " 'alena',\n",
       " 'malaya',\n",
       " 'miley',\n",
       " 'milan',\n",
       " 'sariyah',\n",
       " 'malani',\n",
       " 'clarissa',\n",
       " 'nala',\n",
       " 'princess',\n",
       " 'amani',\n",
       " 'analia',\n",
       " 'estella',\n",
       " 'milana',\n",
       " 'aya',\n",
       " 'chana',\n",
       " 'jayde',\n",
       " 'tenley',\n",
       " 'zaria',\n",
       " 'itzayana',\n",
       " 'penny',\n",
       " 'ailani',\n",
       " 'lara',\n",
       " 'aubriella',\n",
       " 'clare',\n",
       " 'lina',\n",
       " 'rhea',\n",
       " 'bria',\n",
       " 'thalia',\n",
       " 'keyla',\n",
       " 'haisley',\n",
       " 'ryann',\n",
       " 'addisyn',\n",
       " 'amaia',\n",
       " 'chanel',\n",
       " 'ellen',\n",
       " 'harmoni',\n",
       " 'aliana',\n",
       " 'tinsley',\n",
       " 'landry',\n",
       " 'paisleigh',\n",
       " 'lexie',\n",
       " 'myah',\n",
       " 'rylan',\n",
       " 'deborah',\n",
       " 'emilee',\n",
       " 'laylah',\n",
       " 'novalee',\n",
       " 'ellis',\n",
       " 'emmeline',\n",
       " 'avalynn',\n",
       " 'hadlee',\n",
       " 'legacy',\n",
       " 'braylee',\n",
       " 'elisabeth',\n",
       " 'kaylie',\n",
       " 'ansley',\n",
       " 'dior',\n",
       " 'paula',\n",
       " 'belen',\n",
       " 'corinne',\n",
       " 'maleah',\n",
       " 'martha',\n",
       " 'teresa',\n",
       " 'salma',\n",
       " 'louise',\n",
       " 'averi',\n",
       " 'lilianna',\n",
       " 'amiya',\n",
       " 'milena',\n",
       " 'royal',\n",
       " 'aubrielle',\n",
       " 'calliope',\n",
       " 'frankie',\n",
       " 'natasha',\n",
       " 'kamilah',\n",
       " 'meilani',\n",
       " 'raina',\n",
       " 'amayah',\n",
       " 'lailah',\n",
       " 'rayne',\n",
       " 'zaniyah',\n",
       " 'isabela',\n",
       " 'nathalie',\n",
       " 'miah',\n",
       " 'opal',\n",
       " 'kenia',\n",
       " 'azariah',\n",
       " 'hunter',\n",
       " 'tori',\n",
       " 'andi',\n",
       " 'keily',\n",
       " 'leanna',\n",
       " 'scarlette',\n",
       " 'jaelyn',\n",
       " 'saoirse',\n",
       " 'selene',\n",
       " 'dalary',\n",
       " 'lindsey',\n",
       " 'marianna',\n",
       " 'ramona',\n",
       " 'estelle',\n",
       " 'giovanna',\n",
       " 'holland',\n",
       " 'nancy',\n",
       " 'emmalynn',\n",
       " 'mylah',\n",
       " 'rosalee',\n",
       " 'sariah',\n",
       " 'zoie',\n",
       " 'blaire',\n",
       " 'lyanna',\n",
       " 'maxine',\n",
       " 'anais',\n",
       " 'dana',\n",
       " 'judith',\n",
       " 'kiera',\n",
       " 'jaelynn',\n",
       " 'noor',\n",
       " 'kai',\n",
       " 'adalee',\n",
       " 'oaklee',\n",
       " 'amaris',\n",
       " 'jaycee',\n",
       " 'belle',\n",
       " 'carolyn',\n",
       " 'della',\n",
       " 'karter',\n",
       " 'sky',\n",
       " 'treasure',\n",
       " 'vienna',\n",
       " 'jewel',\n",
       " 'rivka',\n",
       " 'rosalyn',\n",
       " 'alannah',\n",
       " 'ellianna',\n",
       " 'sunny',\n",
       " 'claudia',\n",
       " 'cara',\n",
       " 'hailee',\n",
       " 'estrella',\n",
       " 'harleigh',\n",
       " 'zhavia',\n",
       " 'alianna',\n",
       " 'brittany',\n",
       " 'jaylene',\n",
       " 'journi',\n",
       " 'marissa',\n",
       " 'mavis',\n",
       " 'iliana',\n",
       " 'jurnee',\n",
       " 'aislinn',\n",
       " 'alyson',\n",
       " 'elsa',\n",
       " 'kamiyah',\n",
       " 'kiana',\n",
       " 'lisa',\n",
       " 'arlette',\n",
       " 'kadence',\n",
       " 'kathleen',\n",
       " 'halle',\n",
       " 'erika',\n",
       " 'sylvie',\n",
       " 'adele',\n",
       " 'erica',\n",
       " 'veda',\n",
       " 'whitney',\n",
       " 'bexley',\n",
       " 'emmaline',\n",
       " 'guadalupe',\n",
       " 'august',\n",
       " 'brynleigh',\n",
       " 'gwen',\n",
       " 'promise',\n",
       " 'alisson',\n",
       " 'india',\n",
       " 'madalynn',\n",
       " 'paloma',\n",
       " 'patricia',\n",
       " 'samira',\n",
       " 'aliya',\n",
       " 'casey',\n",
       " 'jazlynn',\n",
       " 'paulina',\n",
       " 'dulce',\n",
       " 'kallie',\n",
       " 'perla',\n",
       " 'adrienne',\n",
       " 'alora',\n",
       " 'nataly',\n",
       " 'ayleen',\n",
       " 'christine',\n",
       " 'kaiya',\n",
       " 'ariadne',\n",
       " 'karlee',\n",
       " 'barbara',\n",
       " 'lillianna',\n",
       " 'raquel',\n",
       " 'saniyah',\n",
       " 'yamileth',\n",
       " 'arely',\n",
       " 'celia',\n",
       " 'heavenly',\n",
       " 'kaylin',\n",
       " 'marisol',\n",
       " 'marleigh',\n",
       " 'avalyn',\n",
       " 'berkley',\n",
       " 'kataleya',\n",
       " 'zainab',\n",
       " 'dani',\n",
       " 'egypt',\n",
       " 'joyce',\n",
       " 'kenley',\n",
       " 'annabel',\n",
       " 'kaelyn',\n",
       " 'etta',\n",
       " 'hadleigh',\n",
       " 'joselyn',\n",
       " 'luella',\n",
       " 'jaylee',\n",
       " 'zola',\n",
       " 'alisha',\n",
       " 'ezra',\n",
       " 'queen',\n",
       " 'amia',\n",
       " 'annalee',\n",
       " 'bellamy',\n",
       " 'paola',\n",
       " 'tinley',\n",
       " 'violeta',\n",
       " 'jenesis',\n",
       " 'arden',\n",
       " 'giana',\n",
       " 'wendy',\n",
       " 'ellison',\n",
       " 'florence',\n",
       " 'margo',\n",
       " 'naya',\n",
       " 'robin',\n",
       " 'sandra',\n",
       " 'scout',\n",
       " 'waverly',\n",
       " 'janessa',\n",
       " 'jayden',\n",
       " 'micah',\n",
       " 'novah',\n",
       " 'zora',\n",
       " 'ann',\n",
       " 'jana',\n",
       " 'taliyah',\n",
       " 'vada',\n",
       " 'giavanna',\n",
       " 'ingrid',\n",
       " 'valery',\n",
       " 'azaria',\n",
       " 'emmarie',\n",
       " 'esperanza',\n",
       " 'kailyn',\n",
       " 'aiyana',\n",
       " 'keilani',\n",
       " 'austyn',\n",
       " 'whitley',\n",
       " 'elina',\n",
       " 'kimora',\n",
       " 'maliah',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b413db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = set(''.join(words))\n",
    "chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be2dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_items([('f', 1), ('t', 2), ('c', 3), ('k', 4), ('g', 5), ('d', 6), ('h', 7), ('r', 8), ('y', 9), ('p', 10), ('o', 11), ('u', 12), ('n', 13), ('q', 14), ('i', 15), ('s', 16), ('z', 17), ('m', 18), ('b', 19), ('x', 20), ('a', 21), ('v', 22), ('l', 23), ('w', 24), ('j', 25), ('e', 26), ('.', 0)]),\n",
       " dict_items([(1, 'f'), (2, 't'), (3, 'c'), (4, 'k'), (5, 'g'), (6, 'd'), (7, 'h'), (8, 'r'), (9, 'y'), (10, 'p'), (11, 'o'), (12, 'u'), (13, 'n'), (14, 'q'), (15, 'i'), (16, 's'), (17, 'z'), (18, 'm'), (19, 'b'), (20, 'x'), (21, 'a'), (22, 'v'), (23, 'l'), (24, 'w'), (25, 'j'), (26, 'e'), (0, '.')]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {c: i +1 for i,c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "stoi.items(), itos.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280ad4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 26, 18, 18, 21,  0, 11, 23, 15, 22]),\n",
       " tensor([26, 18, 18, 21,  0, 11, 23, 15, 22, 15]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs , ys = [], [] \n",
    "\n",
    "for w in words :\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1 , ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs[0:10], ys[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e65461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont send the indexes to the model directly, we need to convert them to one-hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7835f55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7b5230b1a720>,\n",
       " torch.Size([228146, 27]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "xenc= F.one_hot(xs, num_classes=27).float()\n",
    "\n",
    "plt.imshow(xenc[:10], cmap='gray', aspect='auto'),xenc.shape, xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b27ac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]),\n",
       " torch.float32,\n",
       " tensor(1.1614, grad_fn=<SelectBackward0>),\n",
       " tensor(1.1614, grad_fn=<DotBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "W = torch.randn((27, 27),generator=g,requires_grad=True) # each column is weights for a neuron \n",
    "multiplied = xenc @ W # matrix multiplication\n",
    "multiplied.shape, multiplied.dtype , multiplied[3][13], xenc[3]@ W[:,13] # this is the activation of the 14th neuron for the 4th input \n",
    "# for the 4th input what is the activation of the 14th neuron\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1369f",
   "metadata": {},
   "source": [
    "# Understanding Logits in Neural Networks\n",
    "\n",
    "Logits are the raw output values from a neural network layer before applying an activation function like softmax. In this notebook, we calculate logits as:\n",
    "\n",
    "```\n",
    "multiplied = xenc @ W  # matrix multiplication\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `xenc` is a one-hot encoded representation of our input characters (shape: [n_samples, 27])\n",
    "- `W` is our weight matrix (shape: [27, 27])\n",
    "- `multiplied` contains the logits (shape: [n_samples, 27])\n",
    "\n",
    "Each row in the logits matrix represents the unnormalized scores for each possible next character. The higher the logit value, the more likely that character is predicted to follow the input character.\n",
    "\n",
    "To convert these logits into probabilities, we would typically apply the softmax function:\n",
    "\n",
    "```\n",
    "probs = softmax(logits)  # transforms logits to probabilities that sum to 1\n",
    "```\n",
    "\n",
    "The highest logit will correspond to the model's prediction for the next character in the sequence.\n",
    "\n",
    "## Relationship Between Logits and Counts\n",
    "\n",
    "In this notebook, we compute the exponentiated logits:\n",
    "\n",
    "```\n",
    "counts = logits.exp()\n",
    "```\n",
    "\n",
    "This exponential transformation is the first step of the softmax function. The exponential ensures all values are positive and amplifies differences between logits - larger logits produce exponentially larger counts.\n",
    "\n",
    "The full softmax would normalize these counts to obtain probabilities:\n",
    "```\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "```\n",
    "\n",
    "Why this matters:\n",
    "- Exponentiating preserves the ranking of predictions (highest logit → highest probability)\n",
    "- It transforms unbounded logits (-∞ to +∞) to strictly positive values\n",
    "- The relative magnitudes between counts directly reflect the model's confidence\n",
    "- Without normalization, counts can exceed 1 and don't sum to 1\n",
    "- After normalization, we get a proper probability distribution over all possible next characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d407ba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]), tensor(3.7159, grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log counts for each next character\n",
    "counts = logits.exp() # convert to counts more like N matrix which has showing how many times each character appears as next character of a particular character\n",
    "probs = counts / counts.sum(1, keepdim=True) # convert to probabilities\n",
    "probs.sum(1, keepdim=True) # check that the probabilities sum to 1 \n",
    "# the above one is the softmax function\n",
    "loss = -(probs[torch.arange(len(xs)), ys]).log().mean() # negative log likelihood loss\n",
    "probs.shape,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80981cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0], probs[0] # for the first input, what is the one-hot encoding and what are the probabilities of each character being next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b28b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,26)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 26\n",
      "probability assigned by the net to the the correct character: 0.14589925110340118\n",
      "log likelihood: -1.9248390197753906\n",
      "negative log likelihood: 1.9248390197753906\n",
      "--------\n",
      "bigram example 2: em (indexes 26,18)\n",
      "input to the neural net: 26\n",
      "output probabilities from the neural net: tensor([0.0634, 0.0270, 0.0101, 0.0040, 0.0188, 0.0129, 0.0115, 0.0283, 0.0220,\n",
      "        0.0805, 0.0116, 0.0218, 0.0112, 0.0253, 0.0204, 0.0370, 0.0389, 0.0270,\n",
      "        0.0548, 0.0370, 0.0471, 0.0335, 0.0307, 0.1727, 0.0833, 0.0175, 0.0517],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 18\n",
      "probability assigned by the net to the the correct character: 0.05480797216296196\n",
      "log likelihood: -2.9039196968078613\n",
      "negative log likelihood: 2.9039196968078613\n",
      "--------\n",
      "bigram example 3: mm (indexes 18,18)\n",
      "input to the neural net: 18\n",
      "output probabilities from the neural net: tensor([0.0339, 0.0600, 0.0216, 0.0135, 0.0521, 0.0291, 0.0260, 0.0082, 0.1074,\n",
      "        0.0920, 0.0097, 0.0033, 0.0549, 0.0896, 0.0220, 0.0732, 0.0292, 0.0292,\n",
      "        0.0130, 0.0376, 0.0153, 0.0062, 0.0019, 0.0186, 0.0082, 0.0185, 0.1260],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 18\n",
      "probability assigned by the net to the the correct character: 0.012968973256647587\n",
      "log likelihood: -4.345195293426514\n",
      "negative log likelihood: 4.345195293426514\n",
      "--------\n",
      "bigram example 4: ma (indexes 18,21)\n",
      "input to the neural net: 18\n",
      "output probabilities from the neural net: tensor([0.0339, 0.0600, 0.0216, 0.0135, 0.0521, 0.0291, 0.0260, 0.0082, 0.1074,\n",
      "        0.0920, 0.0097, 0.0033, 0.0549, 0.0896, 0.0220, 0.0732, 0.0292, 0.0292,\n",
      "        0.0130, 0.0376, 0.0153, 0.0062, 0.0019, 0.0186, 0.0082, 0.0185, 0.1260],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 21\n",
      "probability assigned by the net to the the correct character: 0.006176528055220842\n",
      "log likelihood: -5.08699893951416\n",
      "negative log likelihood: 5.08699893951416\n",
      "--------\n",
      "bigram example 5: a. (indexes 21,0)\n",
      "input to the neural net: 21\n",
      "output probabilities from the neural net: tensor([0.0328, 0.0121, 0.0021, 0.1553, 0.0130, 0.0116, 0.1672, 0.0198, 0.0132,\n",
      "        0.0540, 0.0179, 0.0059, 0.0063, 0.0299, 0.0180, 0.0295, 0.0123, 0.0111,\n",
      "        0.0081, 0.1111, 0.0208, 0.0099, 0.0081, 0.0230, 0.0130, 0.1100, 0.0836],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.03278186172246933\n",
      "log likelihood: -3.417879819869995\n",
      "negative log likelihood: 3.417879819869995\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.5357666015625\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b094c2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5358, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(5), ys[:5]].log().mean() # this is the same as above, but using the probabilities directly\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2c58dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "W.grad = None # reset the gradients\n",
    "loss.backward() # compute the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a0e1cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8.5078e-03, -4.2405e-04, -4.0004e-03, -6.1661e-03, -1.0614e-02,\n",
       "          -1.2045e-03, -7.0281e-03, -5.7299e-04, -5.2561e-03,  2.0485e-03,\n",
       "          -1.1462e-03,  2.1785e-03,  9.3922e-04, -3.8680e-03,  6.6054e-03,\n",
       "           3.0493e-02, -5.3154e-04, -3.7216e-03, -7.6176e-03, -4.9556e-03,\n",
       "           4.1710e-03, -1.7790e-02, -1.2426e-03, -4.1025e-03,  3.1220e-04,\n",
       "           1.0884e-02,  1.4103e-02],\n",
       "         [-2.9123e-04, -1.5857e-04,  7.8336e-05,  3.9729e-05,  2.3160e-04,\n",
       "           1.1782e-04,  4.3009e-04,  4.7676e-05, -4.5007e-04, -4.2299e-05,\n",
       "           4.0619e-04, -2.2890e-04,  3.4807e-04,  2.6715e-05,  9.2062e-05,\n",
       "          -6.1928e-04,  1.3561e-04,  2.2059e-05,  3.5672e-04,  2.1081e-04,\n",
       "           1.8365e-04, -9.3814e-04,  2.0380e-05,  4.2682e-05,  2.4170e-04,\n",
       "           1.9946e-04, -5.0289e-04],\n",
       "         [-1.5096e-03,  1.8388e-04, -8.7284e-04,  7.6651e-04,  2.1086e-04,\n",
       "           2.3070e-04,  5.9970e-04, -2.4116e-03, -1.3577e-03, -1.3819e-03,\n",
       "           1.3376e-03, -2.7068e-03,  2.6590e-03,  4.3295e-03,  2.2259e-04,\n",
       "          -1.4317e-03,  8.1750e-04,  1.9251e-03,  1.6680e-04,  2.6532e-04,\n",
       "           2.4529e-04, -1.4830e-03,  5.5859e-04,  8.2659e-04,  2.3145e-04,\n",
       "           5.5275e-04, -2.9746e-03],\n",
       "         [ 1.5108e-03,  4.2441e-04, -1.1121e-05,  1.8700e-03, -6.1647e-04,\n",
       "           1.5581e-03,  2.3045e-04, -2.7521e-03,  1.0934e-03, -3.0806e-04,\n",
       "           6.3979e-04, -1.0066e-03, -3.4099e-05,  2.6874e-04,  1.7066e-03,\n",
       "          -9.5578e-04,  3.9115e-04,  1.2374e-04,  6.2407e-04,  2.0085e-04,\n",
       "           2.2756e-04, -3.5045e-03,  4.2875e-04, -4.0830e-04,  3.9680e-04,\n",
       "           7.6428e-05, -2.1746e-03],\n",
       "         [-1.4203e-03,  1.2958e-03,  3.9288e-04,  1.4065e-03,  8.9267e-04,\n",
       "           8.3297e-05,  3.9596e-04, -7.3170e-04, -3.8479e-04, -7.7049e-04,\n",
       "           2.0265e-03,  1.2306e-03,  2.5007e-04,  6.0293e-04,  7.1654e-04,\n",
       "          -2.1300e-03, -4.3912e-05,  1.5002e-03,  1.4826e-04,  4.8506e-04,\n",
       "           1.4459e-03, -5.4182e-03,  5.9387e-04, -3.4584e-06, -5.9954e-06,\n",
       "           1.5185e-04, -2.7101e-03],\n",
       "         [-2.2851e-04,  6.6821e-04,  7.3844e-05,  4.4012e-04,  1.6794e-03,\n",
       "           1.3427e-04, -4.0927e-06, -1.2950e-03, -7.9903e-04,  1.1838e-04,\n",
       "           5.9258e-04, -1.7078e-04, -2.7578e-04,  3.4129e-05,  9.1532e-05,\n",
       "          -5.6663e-04,  1.1390e-04,  3.3771e-05,  7.4733e-04,  1.6817e-04,\n",
       "           4.1022e-04, -1.1930e-03,  4.1891e-04, -1.1757e-04, -1.4400e-05,\n",
       "           5.0689e-06, -1.0650e-03],\n",
       "         [-9.3489e-04,  4.6861e-04,  6.3545e-03,  2.5789e-04,  7.5587e-04,\n",
       "           4.6247e-04, -3.3826e-04, -3.2746e-04,  2.3388e-04, -5.8969e-04,\n",
       "           1.0079e-03, -1.1426e-03, -1.1826e-04,  1.4180e-04,  1.5370e-04,\n",
       "          -2.5304e-03,  2.3172e-03,  3.9817e-04,  1.2397e-03,  1.9803e-04,\n",
       "           1.0948e-04, -4.9566e-03,  1.8210e-04,  1.2327e-04,  5.7385e-04,\n",
       "           8.6674e-04, -4.9069e-03],\n",
       "         [-1.0323e-02,  6.3746e-04,  1.0195e-03,  1.1001e-03,  5.5780e-03,\n",
       "           6.1959e-04,  2.5583e-04,  1.0450e-03, -6.3061e-04, -4.2515e-04,\n",
       "           4.8262e-04,  1.5737e-03,  2.7222e-03,  1.8447e-03,  2.1583e-03,\n",
       "          -2.9627e-03,  1.5365e-03,  1.7167e-03, -1.3056e-04,  1.3745e-03,\n",
       "           1.8568e-04, -9.2105e-03,  1.7617e-03, -7.3460e-05,  2.6448e-04,\n",
       "           4.5118e-04, -2.5714e-03],\n",
       "         [-3.2813e-03,  1.0804e-03,  3.7548e-03,  1.4610e-03,  4.7284e-03,\n",
       "           1.2266e-04, -2.3285e-04,  1.1137e-03, -1.1998e-03, -2.4648e-03,\n",
       "           3.2621e-04,  7.3154e-04, -8.5886e-04,  2.2948e-04,  4.0397e-04,\n",
       "          -1.2115e-02,  1.9662e-04,  4.2439e-03, -1.4271e-05,  2.4192e-03,\n",
       "           2.4180e-03, -9.9894e-04,  5.7972e-04, -4.4074e-04,  2.1709e-03,\n",
       "           2.7916e-03, -7.1659e-03],\n",
       "         [-7.8210e-03,  4.4844e-03,  4.1063e-04,  9.0329e-04,  2.7420e-03,\n",
       "           4.9788e-04, -4.2334e-04,  2.2168e-04, -2.0364e-05,  1.1702e-03,\n",
       "           7.0834e-04, -4.2142e-04,  1.3954e-03, -7.7496e-03,  4.1266e-03,\n",
       "          -5.8876e-04,  1.6550e-03,  3.0246e-03,  4.0050e-04,  4.2579e-03,\n",
       "           1.2089e-03, -7.9261e-03,  1.1092e-03, -4.4118e-03,  9.0632e-04,\n",
       "           9.7201e-04, -8.3273e-04],\n",
       "         [ 2.5792e-05,  2.2296e-04,  2.5429e-04,  5.9749e-05,  9.3600e-05,\n",
       "           6.8856e-05,  5.6302e-05, -7.7045e-04, -6.2964e-04,  2.8572e-04,\n",
       "          -3.2828e-05, -9.9253e-05,  5.8556e-05,  4.8246e-04,  6.6370e-05,\n",
       "          -5.4279e-05, -8.2847e-06,  4.1306e-04,  1.8445e-05,  3.5195e-04,\n",
       "           5.3712e-05, -6.5119e-04,  1.3010e-04,  1.7531e-04,  5.6643e-05,\n",
       "           1.2329e-04, -7.5125e-04],\n",
       "         [-3.5207e-03,  1.4020e-03, -3.3647e-04, -1.8266e-04,  2.4392e-03,\n",
       "           2.3290e-04,  1.3262e-03,  1.6007e-03, -2.7512e-03,  5.6120e-05,\n",
       "          -1.8570e-04,  1.6581e-03,  3.1209e-03, -9.8594e-03,  4.9000e-04,\n",
       "           1.4435e-03, -1.0204e-03,  4.6173e-04,  1.2563e-04,  7.2867e-04,\n",
       "           2.0159e-03, -7.9619e-05,  1.3756e-03, -1.5204e-03,  9.0973e-04,\n",
       "           1.6627e-05,  5.2948e-05],\n",
       "         [-5.3238e-04,  1.5443e-04,  2.5202e-04, -3.5859e-04, -2.5738e-05,\n",
       "           1.1249e-04, -4.3187e-04,  1.9826e-04, -1.2911e-03,  1.2831e-04,\n",
       "           3.0412e-07,  7.2497e-05,  4.8842e-04, -8.8388e-04,  5.2090e-04,\n",
       "          -1.3210e-04,  4.5778e-04,  8.4280e-04, -5.2361e-04, -2.5558e-04,\n",
       "           2.0899e-04,  1.0225e-03,  3.0244e-04, -3.0649e-04,  2.5571e-05,\n",
       "           4.6527e-04, -5.1171e-04],\n",
       "         [-2.7123e-02,  5.8671e-03,  1.9507e-03,  1.7388e-03,  5.1573e-03,\n",
       "           4.1440e-04, -9.6782e-04,  1.8892e-03,  9.6399e-03, -7.1885e-04,\n",
       "           5.7912e-04,  4.1645e-03,  6.3212e-04, -6.2061e-03,  1.1723e-03,\n",
       "          -6.6564e-03,  3.4804e-03,  3.3919e-04,  5.1354e-03,  4.3285e-04,\n",
       "           1.6427e-03, -1.2416e-02,  8.2545e-04,  7.8063e-04,  9.6145e-03,\n",
       "           3.5707e-03, -4.9392e-03],\n",
       "         [ 6.2827e-05,  2.2642e-05,  1.2442e-04,  4.6903e-05,  4.0863e-05,\n",
       "           3.7903e-06,  1.1132e-05,  3.7760e-05,  1.6740e-05,  4.7084e-05,\n",
       "           1.2806e-05,  1.8213e-05, -8.8848e-04,  1.0404e-04,  2.8209e-05,\n",
       "          -4.3716e-05,  1.4299e-04,  1.6122e-05,  1.2758e-05,  7.9761e-05,\n",
       "           1.2698e-05, -2.9578e-05,  1.2298e-05,  8.8588e-05,  2.8740e-06,\n",
       "           7.2496e-06,  9.0149e-06],\n",
       "         [-1.0044e-02,  6.3930e-03, -1.9551e-03,  1.7359e-03,  1.1208e-03,\n",
       "          -1.1858e-03,  3.0041e-03,  1.4893e-02, -2.4574e-03, -2.5394e-03,\n",
       "          -1.9758e-06,  3.8771e-03,  3.6039e-03, -6.9151e-03,  1.2924e-03,\n",
       "           5.7627e-04, -3.4472e-03, -2.0642e-04, -1.8250e-04, -2.7284e-04,\n",
       "           2.0765e-03, -7.2462e-03, -1.2694e-04, -5.5520e-03,  8.0484e-04,\n",
       "           1.1445e-03,  1.6107e-03],\n",
       "         [-4.1419e-03,  2.3869e-03, -1.9210e-03,  2.8643e-03,  1.4422e-03,\n",
       "           7.0005e-04,  1.3759e-03, -2.8016e-03,  1.0857e-03, -1.1742e-04,\n",
       "           1.3278e-03, -1.3955e-03,  2.3897e-04,  1.0838e-03,  1.0472e-04,\n",
       "          -1.2164e-03, -4.7159e-04,  1.3711e-04,  1.5411e-04,  6.0102e-04,\n",
       "           8.1787e-04, -4.6852e-03,  4.4360e-04, -7.3859e-04,  4.6579e-03,\n",
       "           1.4140e-03, -3.3469e-03],\n",
       "         [-2.3796e-05,  3.9056e-04,  4.2624e-04,  1.5678e-05,  6.2592e-05,\n",
       "           1.4438e-04,  4.0421e-04, -1.1182e-04,  1.5012e-04, -6.1650e-04,\n",
       "           1.6111e-04, -3.6984e-04,  2.6987e-04,  5.3966e-05,  6.3027e-05,\n",
       "          -9.6436e-04,  1.2101e-03,  3.2710e-04,  1.1069e-03,  3.3760e-04,\n",
       "           1.0660e-04, -3.6568e-03,  1.0281e-04,  7.8315e-05,  1.0936e-04,\n",
       "           1.5946e-03, -1.3721e-03],\n",
       "         [-1.2749e-03,  1.7414e-03,  6.1082e-04,  1.7068e-04,  1.5136e-03,\n",
       "           8.4744e-04,  6.5254e-04,  2.1722e-04,  2.6932e-03,  1.4174e-03,\n",
       "           1.1499e-04, -1.8858e-03,  9.8856e-04,  2.5177e-03,  6.4253e-04,\n",
       "          -3.3733e-03,  6.9633e-04,  8.0287e-04, -3.5042e-04,  6.0446e-04,\n",
       "           4.4639e-04, -1.1168e-02,  4.1097e-05,  5.2020e-04,  2.3116e-04,\n",
       "           5.0829e-04,  7.3783e-05],\n",
       "         [-3.8289e-04,  1.4066e-04,  3.7791e-04,  7.8322e-04,  1.2582e-04,\n",
       "           6.6954e-04, -1.6481e-04, -5.7879e-05, -3.5255e-03,  9.6188e-06,\n",
       "           8.1038e-05, -4.2022e-04,  1.1757e-03,  3.1572e-04,  1.1011e-04,\n",
       "          -7.9693e-04,  4.3118e-04,  8.5839e-05,  2.0127e-04, -1.2392e-04,\n",
       "           6.0853e-04, -7.6650e-04,  1.2326e-04, -2.2136e-04,  4.5147e-04,\n",
       "           3.9351e-04,  3.7556e-04],\n",
       "         [-1.2370e-05, -8.7004e-06, -2.7276e-04,  2.9684e-05,  1.9324e-04,\n",
       "           2.4461e-05,  2.4887e-06,  3.1237e-04,  2.0555e-05, -4.1761e-05,\n",
       "           8.2110e-05,  3.6180e-05, -4.1166e-06,  5.2490e-05,  3.7889e-05,\n",
       "          -2.9288e-04,  1.0654e-04, -3.4902e-05,  5.2259e-05,  1.3689e-04,\n",
       "          -1.4433e-05, -4.0171e-04,  6.9395e-05, -7.5597e-05,  1.1438e-04,\n",
       "           1.7246e-05, -1.2894e-04],\n",
       "         [-2.4102e-02,  1.2104e-03, -2.7019e-03,  2.0912e-02, -5.4677e-04,\n",
       "           9.8945e-04,  2.0157e-02, -7.2705e-03, -1.2329e-02, -9.5213e-04,\n",
       "           2.3075e-03,  6.0672e-04, -7.2455e-04, -1.9363e-02,  2.4215e-03,\n",
       "          -2.8334e-03, -3.0688e-03, -2.5759e-04, -5.9505e-03,  1.4090e-02,\n",
       "           2.3035e-03, -9.5713e-04, -2.4414e-03, -7.6438e-03,  1.2362e-03,\n",
       "           1.5532e-02,  9.3760e-03],\n",
       "         [ 2.5330e-04,  2.6546e-04,  1.1986e-03,  1.3270e-04,  2.2355e-04,\n",
       "           5.4610e-05,  6.0415e-04,  1.8494e-04, -5.9482e-05,  6.6065e-04,\n",
       "           1.2051e-04, -1.6684e-04,  1.2573e-04,  1.8066e-03,  1.3844e-04,\n",
       "          -3.4984e-03,  7.7209e-04,  4.8580e-04,  1.0343e-04,  5.7280e-04,\n",
       "           1.1888e-04, -2.5901e-03,  1.3201e-04,  2.4308e-04,  2.3802e-04,\n",
       "           2.7533e-04, -2.3958e-03],\n",
       "         [ 3.1979e-03,  8.6069e-04, -3.1891e-05,  2.9698e-03,  3.4857e-03,\n",
       "           1.4397e-03,  1.0307e-03,  2.8893e-03,  4.4128e-04, -5.4312e-03,\n",
       "           7.5204e-04, -2.0908e-03, -9.9402e-05,  4.7311e-03,  5.1276e-04,\n",
       "          -1.0374e-02,  3.5026e-03,  6.1528e-04,  8.1593e-04,  3.2652e-04,\n",
       "           8.6304e-03, -9.2611e-03,  1.4197e-03, -5.4616e-03,  3.7481e-03,\n",
       "           1.8736e-03, -1.0493e-02],\n",
       "         [-9.8291e-05,  3.7141e-05,  3.7456e-05,  1.4015e-04, -6.5968e-06,\n",
       "           2.9287e-04,  1.0615e-04,  1.2404e-05,  4.3659e-05, -2.1355e-04,\n",
       "           4.1834e-04, -1.1422e-04, -3.0977e-05, -2.1311e-04,  4.2441e-05,\n",
       "          -6.1440e-04, -3.7223e-06,  2.0208e-04,  1.1222e-04,  3.0027e-04,\n",
       "           5.5234e-04, -1.1421e-03,  1.1873e-04,  3.2878e-04,  6.8784e-05,\n",
       "           4.7405e-05, -4.2422e-04],\n",
       "         [ 7.1190e-05,  1.0121e-04,  1.0567e-04,  1.7459e-04,  1.6666e-03,\n",
       "           1.2818e-04,  2.4459e-04,  2.1987e-03,  7.3184e-04,  5.5160e-05,\n",
       "           9.5644e-05, -1.9979e-03, -7.0645e-04,  6.1570e-05,  6.9481e-05,\n",
       "          -3.8414e-04,  4.9497e-04,  8.8354e-05,  1.8924e-04,  1.2703e-04,\n",
       "           2.2094e-03, -5.9036e-03,  5.5586e-05,  1.1125e-03,  1.3228e-04,\n",
       "           6.6614e-04, -1.7878e-03],\n",
       "         [-1.1779e-02,  2.0551e-03, -1.6347e-03, -3.1321e-04,  8.9854e-04,\n",
       "           6.1040e-04, -6.5121e-04,  1.8683e-03, -6.6135e-03,  2.5055e-03,\n",
       "           6.7042e-04,  7.7152e-04,  7.0446e-04, -9.4602e-03,  1.7645e-03,\n",
       "          -2.7639e-04, -2.9188e-04,  1.6232e-03,  1.6292e-03,  2.7789e-03,\n",
       "           3.6331e-03,  1.9441e-05,  7.1996e-04,  1.1731e-03,  7.2157e-03,\n",
       "           1.3284e-03, -9.4951e-04]]),\n",
       " torch.Size([27, 27]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad,W.grad.shape # these are the gradients of the loss with respect to the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4211bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad # update the weights using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5241ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7159416675567627"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29e9088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3db2f",
   "metadata": {},
   "source": [
    "## Regularization: Intuitive and Mathematical Explanation\n",
    "\n",
    "**Intuitive Explanation**\n",
    "\n",
    "Regularization is a technique used to prevent overfitting in machine learning models. Overfitting happens when a model learns the training data too well, including its noise and outliers, which hurts its performance on new, unseen data. Regularization discourages the model from fitting the noise by adding a penalty for large or complex weights, encouraging simpler models that generalize better.\n",
    "\n",
    "Think of regularization as a way to \"smooth out\" the model, making it less sensitive to small fluctuations in the training data.\n",
    "\n",
    "**Mathematical Explanation**\n",
    "\n",
    "In this notebook, regularization is added to the loss function as follows:\n",
    "\n",
    "\\[\n",
    "\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{i, y_i} + \\lambda \\|W\\|^2\n",
    "\\]\n",
    "\n",
    "- The first term is the average negative log likelihood (cross-entropy loss).\n",
    "- The second term, \\(\\lambda \\|W\\|^2\\), is the regularization penalty (L2 regularization), where:\n",
    "    - \\(W\\) is the weight matrix,\n",
    "    - \\(\\|W\\|^2\\) is the sum of the squares of all weights,\n",
    "    - \\(\\lambda\\) (e.g., 0.01) controls the strength of the penalty.\n",
    "\n",
    "This penalty discourages large weights, making the model less likely to overfit. The higher the value of \\(\\lambda\\), the stronger the regularization effect.\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Term in Loss Function                | Purpose                                 |\n",
    "|--------------------------------------|-----------------------------------------|\n",
    "| \\(-\\log p_{i, y_i}\\)                 | Fit the data (maximize likelihood)      |\n",
    "| \\(+\\lambda \\|W\\|^2\\)                 | Penalize large weights (regularization) |\n",
    "\n",
    "Regularization helps the model generalize better by balancing fit and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561647b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7292466163635254\n",
      "3.3365888595581055\n",
      "3.132108211517334\n",
      "3.0035903453826904\n",
      "2.9103708267211914\n",
      "2.8404126167297363\n",
      "2.787395477294922\n",
      "2.746760845184326\n",
      "2.7151215076446533\n",
      "2.6899852752685547\n",
      "2.6695570945739746\n",
      "2.6525814533233643\n",
      "2.638195514678955\n",
      "2.6258041858673096\n",
      "2.6149933338165283\n",
      "2.6054677963256836\n",
      "2.5970101356506348\n",
      "2.5894556045532227\n",
      "2.5826752185821533\n",
      "2.5765647888183594\n",
      "2.571037530899048\n",
      "2.5660204887390137\n",
      "2.5614521503448486\n",
      "2.5572783946990967\n",
      "2.5534534454345703\n",
      "2.5499372482299805\n",
      "2.546696424484253\n",
      "2.5436995029449463\n",
      "2.5409226417541504\n",
      "2.538342237472534\n",
      "2.5359387397766113\n",
      "2.5336954593658447\n",
      "2.53159761428833\n",
      "2.5296313762664795\n",
      "2.5277862548828125\n",
      "2.5260508060455322\n",
      "2.52441668510437\n",
      "2.5228753089904785\n",
      "2.5214195251464844\n",
      "2.5200419425964355\n",
      "2.518737316131592\n",
      "2.517500162124634\n",
      "2.5163252353668213\n",
      "2.5152082443237305\n",
      "2.5141453742980957\n",
      "2.513132333755493\n",
      "2.5121660232543945\n",
      "2.5112433433532715\n",
      "2.5103611946105957\n",
      "2.5095174312591553\n",
      "2.508709669113159\n",
      "2.5079352855682373\n",
      "2.5071921348571777\n",
      "2.506479024887085\n",
      "2.505793809890747\n",
      "2.5051350593566895\n",
      "2.5045008659362793\n",
      "2.5038902759552\n",
      "2.5033023357391357\n",
      "2.502735137939453\n",
      "2.502188205718994\n",
      "2.5016603469848633\n",
      "2.501150369644165\n",
      "2.500657558441162\n",
      "2.500181198120117\n",
      "2.499720811843872\n",
      "2.499274969100952\n",
      "2.4988436698913574\n",
      "2.4984261989593506\n",
      "2.498021364212036\n",
      "2.497628927230835\n",
      "2.497248888015747\n",
      "2.496880054473877\n",
      "2.4965224266052246\n",
      "2.496175527572632\n",
      "2.4958386421203613\n",
      "2.495511054992676\n",
      "2.4951934814453125\n",
      "2.494884729385376\n",
      "2.494584798812866\n",
      "2.494293451309204\n",
      "2.4940099716186523\n",
      "2.493734359741211\n",
      "2.4934656620025635\n",
      "2.4932055473327637\n",
      "2.4929518699645996\n",
      "2.4927046298980713\n",
      "2.492464065551758\n",
      "2.49222993850708\n",
      "2.492002248764038\n",
      "2.4917805194854736\n",
      "2.4915637969970703\n",
      "2.4913532733917236\n",
      "2.491147756576538\n",
      "2.490947961807251\n",
      "2.490752935409546\n",
      "2.490562677383423\n",
      "2.490377426147461\n",
      "2.490196704864502\n",
      "2.4900200366973877\n",
      "2.4898483753204346\n",
      "2.489680290222168\n",
      "2.4895167350769043\n",
      "2.4893572330474854\n",
      "2.489201307296753\n",
      "2.4890494346618652\n",
      "2.488900899887085\n",
      "2.488755702972412\n",
      "2.4886138439178467\n",
      "2.4884755611419678\n",
      "2.4883408546447754\n",
      "2.4882090091705322\n",
      "2.4880800247192383\n",
      "2.4879541397094727\n",
      "2.4878311157226562\n",
      "2.487710952758789\n",
      "2.487593412399292\n",
      "2.487478494644165\n",
      "2.4873664379119873\n",
      "2.4872562885284424\n",
      "2.4871490001678467\n",
      "2.487044095993042\n",
      "2.486941337585449\n",
      "2.4868407249450684\n",
      "2.4867424964904785\n",
      "2.4866464138031006\n",
      "2.4865520000457764\n",
      "2.486459970474243\n",
      "2.486370325088501\n",
      "2.486281633377075\n",
      "2.4861953258514404\n",
      "2.4861109256744385\n",
      "2.4860281944274902\n",
      "2.4859468936920166\n",
      "2.485867500305176\n",
      "2.4857892990112305\n",
      "2.485713481903076\n",
      "2.4856386184692383\n",
      "2.485565185546875\n",
      "2.4854936599731445\n",
      "2.4854233264923096\n",
      "2.485354423522949\n",
      "2.4852867126464844\n",
      "2.485220432281494\n",
      "2.4851558208465576\n",
      "2.4850916862487793\n",
      "2.4850292205810547\n",
      "2.4849681854248047\n",
      "2.484907627105713\n",
      "2.484848976135254\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(150):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  # W is more like N matrix  and we make this W using loss function backward # \n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # 0.01 is like regularization strength, this is the loss function we want to minimize\n",
    "  \n",
    "  \n",
    "  # in the bigram model we used +1 to smooth the counts , as N and W are analogous to the bigram counts and W is more like N matrix, we use regularization to avoid overfitting\n",
    "  # we use regularization to avoid overfitting, this is equivalent to adding +1 to the counts in the bigram model\n",
    "  # 0.01 is the regularization strength, you can change it to see how it affects the loss\n",
    "  # w wants to be small or zero if w is zero then the probs  # will be close to uniform distribution\n",
    "  \n",
    "  \n",
    "  # remember \n",
    "  # we used regularization to avoid overfitting \n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10779cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kewen.\n",
      "einion.\n",
      "oliseolirannneynn.\n",
      "cayliuhavo.\n",
      "or.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65afe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
